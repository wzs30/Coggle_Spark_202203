{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark基础教程\n",
    "\n",
    "Spark是一个快速和通用的大数据引擎，可以通俗的理解成一个分布式的大数据处理框架，允许用户将Spark部署在大量廉价的硬件之上，形成集群。Spark使用scala 实现，提供了 JAVA, Python，R等语言的调用接口。在本次学习我们将学习如何使用Spark清洗数据，并进行基础的特征工程操作，帮助大家掌握基础PySpark技能。\n",
    "\n",
    "学习资料：\n",
    "\n",
    "https://spark.apache.org/docs/latest/quick-start.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://github.com/apache/spark/tree/4f25b3f712/examples/src/main/python\n",
    "\n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "https://www.tutorialspoint.com/pyspark/index.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1：PySpark数据处理\n",
    "\n",
    "- 步骤1：使用Python链接Spark环境\n",
    "- 步骤2：创建dateframe数据\n",
    "- 步骤3：用spark执行以下逻辑：找到数据行数、列数\n",
    "- 步骤4：用spark筛选class为1的样本\n",
    "- 步骤5：用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：使用Python链接Spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加此代码，进行spark初始化\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：创建dateframe数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   002|    2|      87|  81|     90|    83|      83|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "|   004|    2|      65|  87|     94|    73|      88|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   006|    3|      84|  82|     85|    73|      99|\n",
      "|   007|    3|      56|  76|     63|    72|      87|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "|   009|    2|      63|  72|     87|    98|      64|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建原始数据 \n",
    "test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "                                ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "                                ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "                            ['number','class','language','math','english','physic','chemical'])\n",
    "# 展示结果\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：用spark执行以下逻辑：找到数据行数、列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:9\n",
      "列数:7\n"
     ]
    }
   ],
   "source": [
    "# 统计行数\n",
    "print(f'行数:{test.count()}')\n",
    "\n",
    "# 统计列数\n",
    "print(f'列数:{len(test.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：用spark筛选class为1的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用fliter过滤样本\n",
    "test.filter(test['class'] == '1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 过滤条件\n",
    "f1 = test['language'] > 90\n",
    "f2 = test['math'] > 90\n",
    "\n",
    "# 使用fliter过滤样本\n",
    "test.filter(f1|f2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2：PySpark数据统计\n",
    "\n",
    "- 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "- 步骤2：将读取的进行保存，表头也需要保存\n",
    "- 步骤3：分析每列的类型，取值个数\n",
    "- 步骤4：分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：读取文件\n",
    "\n",
    "文件链接：https://cdn.coggle.club/Pokemon.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# 从url读取数据\n",
    "spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：将读取的进行保存，表头也需要保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据进行保存\n",
    "df.write.csv(\"./data/Pokemon.csv\", header=True, mode=\"overwrite\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：分析每列的类型，取值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type 1: string (nullable = true)\n",
      " |-- Type 2: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- HP: integer (nullable = true)\n",
      " |-- Attack: integer (nullable = true)\n",
      " |-- Defense: integer (nullable = true)\n",
      " |-- Sp Atk: integer (nullable = true)\n",
      " |-- Sp Def: integer (nullable = true)\n",
      " |-- Speed: integer (nullable = true)\n",
      " |-- Generation: integer (nullable = true)\n",
      " |-- Legendary: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 每列类型\n",
    "df.printSchema()    # df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+------+------------------+------------------+-----------------+------------------+----------------+-----------------+------------------+------------------+\n",
      "|summary|            Name|Type 1|Type 2|             Total|                HP|           Attack|           Defense|          Sp Atk|           Sp Def|             Speed|        Generation|\n",
      "+-------+----------------+------+------+------------------+------------------+-----------------+------------------+----------------+-----------------+------------------+------------------+\n",
      "|  count|             800|   800|   414|               800|               800|              800|               800|             800|              800|               800|               800|\n",
      "|   mean|            null|  null|  null|          435.1025|          69.25875|         79.00125|           73.8425|           72.82|          71.9025|           68.2775|           3.32375|\n",
      "| stddev|            null|  null|  null|119.96303975551908|25.534669032332076|32.45736586949845|31.183500559332924|32.7222941688016|27.82891579711746|29.060473717161447|1.6612904004849451|\n",
      "|    min|       Abomasnow|   Bug|   Bug|               180|                 1|                5|                 5|              10|               20|                 5|                 1|\n",
      "|    max|Zygarde50% Forme| Water| Water|               780|               255|              190|               230|             194|              230|               180|                 6|\n",
      "+-------+----------------+------+------+------------------+------------------+-----------------+------------------+----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据描述性统计\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "|Name_null|Type 1_null|Type 2_null|Total_null|HP_null|Attack_null|Defense_null|Sp Atk_null|Sp Def_null|Speed_null|Generation_null|Legendary_null|\n",
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "|        0|          0|        386|         0|      0|          0|           0|          0|          0|         0|              0|             0|\n",
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "# 统计各列缺失值数量\n",
    "df.agg(*[(fn.count('*') - fn.count(c)).alias(c+'_null') for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务3：PySpark分组聚合\n",
    "\n",
    "- 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "- 步骤2：学习groupby分组聚合的使用\n",
    "- 步骤3：学习agg分组聚合的使用\n",
    "- 步骤4：学习transform的使用\n",
    "- 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkFiles\n",
    "\n",
    "# # 从url读取数据\n",
    "# spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "# df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "# df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "# df = df.withColumnRenamed('Sp. Def', 'Sp Def')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：学习groupby分组聚合的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html#pyspark.sql.DataFrame.groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：学习agg分组聚合的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：学习transform的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.transform.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  Type 1|          avg(HP)|\n",
      "+--------+-----------------+\n",
      "|   Water|          72.0625|\n",
      "|  Poison|            67.25|\n",
      "|   Steel|65.22222222222223|\n",
      "|    Rock|65.36363636363636|\n",
      "|     Ice|             72.0|\n",
      "|   Ghost|          64.4375|\n",
      "|   Fairy|74.11764705882354|\n",
      "| Psychic|70.63157894736842|\n",
      "|  Dragon|          83.3125|\n",
      "|  Flying|            70.75|\n",
      "|     Bug|56.88405797101449|\n",
      "|Electric|59.79545454545455|\n",
      "|    Fire|69.90384615384616|\n",
      "|  Ground|         73.78125|\n",
      "|    Dark|66.80645161290323|\n",
      "|Fighting|69.85185185185185|\n",
      "|   Grass|67.27142857142857|\n",
      "|  Normal|77.27551020408163|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.groupBy('Type 1').avg('HP').show()  # df.groupBy('Type 1').mean('HP').show()\n",
    "df.groupBy('Type 1').agg({'HP':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务4：SparkSQL基础语法\n",
    "\n",
    "- 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "- 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "- 步骤3：使用Spark SQL完成任务3的分组统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "\n",
    "筛选test数据集中 language>90 或 math>90 的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.SQL风格\n",
    "test.createOrReplaceTempView(\"test\")\n",
    "spark.sql(\"SELECT * FROM test WHERE language>90 or math>90\").show()\n",
    "\n",
    "# # 2.DSL风格——领域特定语言\n",
    "# test.where(\"language>90 or math>90\").show()\n",
    "# test.filter(\"language>90 or math>90\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "\n",
    "统计各列缺失值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将DataFrame注册为临时表，如果存在进行替换\n",
    "df.createOrReplaceTempView(\"pokemon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Type 2_missing|\n",
      "+--------------+\n",
      "|           386|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. SQL风格——以某列为例\n",
    "spark.sql(\"SELECT SUM(CASE WHEN `Type 2` IS NULL THEN 1 END) AS `Type 2_missing` FROM pokemon\").show()\n",
    "\n",
    "# # 2.DSL风格\n",
    "# df.agg(*[(fn.count('*') - fn.count(c)).alias(c+'_null') for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：使用Spark SQL完成任务3的分组统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  Type 1|          avg(HP)|\n",
      "+--------+-----------------+\n",
      "|   Water|          72.0625|\n",
      "|  Poison|            67.25|\n",
      "|   Steel|65.22222222222223|\n",
      "|    Rock|65.36363636363636|\n",
      "|     Ice|             72.0|\n",
      "|   Ghost|          64.4375|\n",
      "|   Fairy|74.11764705882354|\n",
      "| Psychic|70.63157894736842|\n",
      "|  Dragon|          83.3125|\n",
      "|  Flying|            70.75|\n",
      "|     Bug|56.88405797101449|\n",
      "|Electric|59.79545454545455|\n",
      "|    Fire|69.90384615384616|\n",
      "|  Ground|         73.78125|\n",
      "|    Dark|66.80645161290323|\n",
      "|Fighting|69.85185185185185|\n",
      "|   Grass|67.27142857142857|\n",
      "|  Normal|77.27551020408163|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.SQL风格\n",
    "spark.sql(\"SELECT `Type 1`, AVG(HP) FROM pokemon GROUP BY `Type 1` \").show()\n",
    "\n",
    "# # 2.DSL风格\n",
    "# df.groupBy('Type 1').agg({'HP':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务5：SparkML基础：数据编码\n",
    "\n",
    "- 步骤1：学习Spark ML中数据编码模块\n",
    "- 步骤2：读取文件Pokemon.csv，理解数据字段含义\n",
    "- 步骤3：将其中的类别属性使用onehotencoder\n",
    "- 步骤4：对其中的数值属性字段使用minmaxscaler\n",
    "- 步骤5：对编码后的属性使用pca进行降维（维度可以自己选择）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：学习Spark ML中数据编码模块\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#feature\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：读取文件Pokemon.csv，理解数据字段含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkFiles\n",
    "\n",
    "# # 从url读取数据\n",
    "# spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "# df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "# df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "# df = df.withColumnRenamed('Sp. Def', 'Sp Def')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：将其中的类别属性使用onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+----------+--------------+--------------+-------------+\n",
      "|                Name|Type 1|Type 2|Generation|      Type1Vec|      Type2Vec|GenerationVec|\n",
      "+--------------------+------+------+----------+--------------+--------------+-------------+\n",
      "|           Bulbasaur| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|             Ivysaur| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|            Venusaur| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|VenusaurMega Venu...| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|          Charmander|  Fire|  null|         1|(18,[5],[1.0])|    (18,[],[])|(6,[1],[1.0])|\n",
      "+--------------------+------+------+----------+--------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# 由于OneHotEncoder不支持string类型，得先对特征进行索引化\n",
    "stringIndexer = StringIndexer(inputCols=['Type 1', 'Type 2'],\n",
    "                            outputCols=['Type1_index', 'Type2_index'],\n",
    "                            stringOrderType='frequencyDesc')\n",
    "indexed = stringIndexer.setHandleInvalid(\"keep\").fit(df).transform(df)\n",
    "\n",
    "# OneHot编码\n",
    "onehotEncoder = OneHotEncoder(inputCols=['Type1_index', 'Type2_index', 'Generation'],\n",
    "                        outputCols=['Type1Vec', 'Type2Vec', 'GenerationVec'])\n",
    "encoded = onehotEncoder.fit(indexed).transform(indexed)\n",
    "\n",
    "encoded.select(['Name', 'Type 1', 'Type 2', 'Generation', 'Type1Vec', 'Type2Vec', 'GenerationVec']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：对其中的数值属性字段使用minmaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                Name|        Total_scaled|           HP_scaled|       Attack_scaled|      Defense_scaled|       Sp Atk_scaled|       Sp Def_scaled|        Speed_scaled|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           Bulbasaur|              [0.23]|[0.1732283464566929]|[0.23783783783783...|[0.19555555555555...|[0.29891304347826...|[0.2142857142857143]|[0.22857142857142...|\n",
      "|             Ivysaur|             [0.375]|[0.23228346456692...|[0.3081081081081081]|[0.2577777777777778]|[0.3804347826086956]|[0.28571428571428...|[0.3142857142857143]|\n",
      "|            Venusaur|[0.5750000000000001]|[0.3110236220472441]|[0.41621621621621...|[0.3466666666666667]|[0.4891304347826087]| [0.380952380952381]|[0.42857142857142...|\n",
      "|VenusaurMega Venu...|[0.7416666666666667]|[0.3110236220472441]|[0.5135135135135136]|[0.5244444444444445]|[0.6086956521739131]|[0.4761904761904762]|[0.42857142857142...|\n",
      "|          Charmander|[0.21500000000000...|[0.14960629921259...|[0.25405405405405...|[0.1688888888888889]|[0.2717391304347826]|[0.14285714285714...|[0.34285714285714...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 由于MinMaxScalar只能对单行进行归一化，因此要利用Pipeline来对转换列表内的每个属性进行转换\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "columns_to_scale = ['Total', 'HP', 'Attack', 'Defense', 'Sp Atk', 'Sp Def', 'Speed']\n",
    "# 向量化要转换的属性\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col+'_vec') for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col+'_vec', outputCol=col+'_scaled') for col in columns_to_scale]\n",
    "# 定义管道\n",
    "pipeline = Pipeline(stages=assemblers+scalers)\n",
    "scaledDf = pipeline.fit(encoded).transform(encoded)\n",
    "\n",
    "scaledDf.select(['Name', 'Total_scaled', 'HP_scaled', 'Attack_scaled', 'Defense_scaled', 'Sp Atk_scaled', 'Sp Def_scaled', 'Speed_scaled']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：对编码后的属性使用pca进行降维（维度可以自己选择）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                Name|        pcaed_vector|\n",
      "+--------------------+--------------------+\n",
      "|           Bulbasaur|[-0.5779908488663...|\n",
      "|             Ivysaur|[-0.8036261416372...|\n",
      "|            Venusaur|[-1.1159009089467...|\n",
      "|VenusaurMega Venu...|[-1.3790872076314...|\n",
      "|          Charmander|[-0.5695573835440...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA \n",
    "\n",
    "# 利用降维后的种族值做PCA主成分分析\n",
    "columns_to_pca = ['Total_scaled', 'HP_scaled', 'Attack_scaled', 'Defense_scaled', 'Sp Atk_scaled', 'Sp Def_scaled', 'Speed_scaled']\n",
    "# 将需要处理的变量以向量形式表示\n",
    "vecAssembler = VectorAssembler(inputCols=columns_to_pca, outputCol='vector_to_pca')\n",
    "pcaDf = vecAssembler.transform(scaledDf)\n",
    "# PCA主成分分析\n",
    "pcaModel = PCA(k=2, inputCol='vector_to_pca', outputCol='pcaed_vector')\n",
    "pcaedDf = pcaModel.fit(pcaDf).transform(pcaDf)\n",
    "\n",
    "pcaedDf.select(['Name', 'pcaed_vector']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务6：SparkML基础：分类模型\n",
    "\n",
    "- 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder\n",
    "- 步骤2：导入合适的标签评价指标，说出选择的原因？\n",
    "- 步骤3：选择至少3种分类方法，完成训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Type1_index|\n",
      "+-----------+\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        5.0|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 之前已经通过StringEncoder转换为标签\n",
    "pcaedDf.select('Type1_index').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：导入合适的标签评价指标，说出选择的原因？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择使用的标签评价指标是：准确率（Accuracy），因为能够最直观展示模型的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：选择至少3种分类方法，完成训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：(607, 3)\n",
      "测试集：(193, 3)\n"
     ]
    }
   ],
   "source": [
    "# 用于训练的数据集\n",
    "data_set = pcaedDf.select(['Type1_index', 'pcaed_vector', 'vector_to_pca'])\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = data_set.randomSplit([0.75, 0.25], seed=2022)\n",
    "# 展示训练集和测试集容量\n",
    "print(f'训练集：({train_df.count()}, {len(train_df.columns)})')\n",
    "print(f'测试集：({test_df.count()}, {len(test_df.columns)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归（Logistic Regression）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[1.82187011299118...|[0.10250281517390...|       8.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[1.83790554968661...|[0.10855677053307...|       4.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[1.89809165873014...|[0.11458183706839...|       4.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[1.98460479629409...|[0.07894981842175...|      14.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[1.91438752031794...|[0.08969628188617...|       4.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 构建模型\n",
    "logi_model = LogisticRegression(featuresCol='pcaed_vector', labelCol='Type1_index').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_logi = logi_model.evaluate(train_df).predictions\n",
    "test_result_logi = logi_model.evaluate(test_df).predictions\n",
    "\n",
    "test_result_logi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：13.23%\n",
      "测试集预测准确率：14.91%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_logi = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_logi)\n",
    "accu_test_logi = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_logi)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_logi:.2%}\\n测试集预测准确率：{accu_test_logi:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树（Decision Trees）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[6.0,5.0,3.0,1.0,...|[0.1,0.0833333333...|       8.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[6.0,5.0,3.0,1.0,...|[0.1,0.0833333333...|       8.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[9.0,11.0,4.0,4.0...|[0.12676056338028...|       5.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[0.0,0.0,0.0,2.0,...|[0.0,0.0,0.0,0.33...|      14.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[0.0,2.0,1.0,3.0,...|[0.0,0.125,0.0625...|       6.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# 构建模型\n",
    "dt_model = DecisionTreeClassifier(maxDepth=5, featuresCol='pcaed_vector', labelCol='Type1_index').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_dt = dt_model.transform(train_df)\n",
    "test_result_dt = dt_model.transform(test_df)\n",
    "\n",
    "test_result_dt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：20.07%\n",
      "测试集预测准确率：15.80%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_dt = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_dt)\n",
    "accu_test_dt = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_dt)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_dt:.2%}\\n测试集预测准确率：{accu_test_dt:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林(Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[0.76928163646085...|[0.03846408182304...|       8.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[0.76928163646085...|[0.03846408182304...|       8.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[1.81797841406816...|[0.09089892070340...|       5.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[0.54745079272573...|[0.02737253963628...|      14.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[0.81204804769046...|[0.04060240238452...|       6.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 构建模型\n",
    "rf_model = RandomForestClassifier(maxDepth=5, featuresCol='pcaed_vector', labelCol='Type1_index').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_rf = rf_model.transform(train_df)\n",
    "test_result_rf = rf_model.transform(test_df)\n",
    "\n",
    "test_result_rf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：28.55%\n",
      "测试集预测准确率：12.15%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_rf = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_rf)\n",
    "accu_test_rf = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_rf)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_rf:.2%}\\n测试集预测准确率：{accu_test_rf:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯（Naive Bayes）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[-9.7607238618814...|[0.13189059172286...|       0.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[-9.6392469119504...|[0.13153579197242...|       0.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[-8.2230245952990...|[0.13073681400775...|       0.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[-8.4232867450927...|[0.13386207166064...|       0.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[-7.8646584468633...|[0.13059304731829...|       0.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# 构建模型\n",
    "bayes_model = NaiveBayes(smoothing=1.0, featuresCol='vector_to_pca', labelCol='Type1_index', modelType='multinomial').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_bayes = bayes_model.transform(train_df)\n",
    "test_result_bayes = bayes_model.transform(test_df)\n",
    "\n",
    "test_result_bayes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：8.83%\n",
      "测试集预测准确率：9.39%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_bayes = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_bayes)\n",
    "accu_test_bayes = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_bayes)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_bayes:.2%}\\n测试集预测准确率：{accu_test_bayes:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务7：SparkML基础：聚类模型\n",
    "\n",
    "- 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder\n",
    "- 步骤2：使用kmeans对宝可梦进行聚类，使用肘部法选择合适聚类个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48de46c395bdbfc127f71df84d0b8cef7bb4c63b5f8ed81eab1950674a230645"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

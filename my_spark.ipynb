{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark基础教程\n",
    "\n",
    "Spark是一个快速和通用的大数据引擎，可以通俗的理解成一个分布式的大数据处理框架，允许用户将Spark部署在大量廉价的硬件之上，形成集群。Spark使用scala 实现，提供了 JAVA, Python，R等语言的调用接口。在本次学习我们将学习如何使用Spark清洗数据，并进行基础的特征工程操作，帮助大家掌握基础PySpark技能。\n",
    "\n",
    "学习资料：\n",
    "\n",
    "https://spark.apache.org/docs/latest/quick-start.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://github.com/apache/spark/tree/4f25b3f712/examples/src/main/python\n",
    "\n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "https://www.tutorialspoint.com/pyspark/index.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1：PySpark数据处理\n",
    "\n",
    "- 步骤1：使用Python链接Spark环境\n",
    "- 步骤2：创建dateframe数据\n",
    "- 步骤3：用spark执行以下逻辑：找到数据行数、列数\n",
    "- 步骤4：用spark筛选class为1的样本\n",
    "- 步骤5：用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：使用Python链接Spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 添加此代码，进行spark初始化\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：创建dateframe数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   002|    2|      87|  81|     90|    83|      83|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "|   004|    2|      65|  87|     94|    73|      88|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   006|    3|      84|  82|     85|    73|      99|\n",
      "|   007|    3|      56|  76|     63|    72|      87|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "|   009|    2|      63|  72|     87|    98|      64|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建原始数据 \n",
    "test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "                                ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "                                ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "                            ['number','class','language','math','english','physic','chemical'])\n",
    "# 展示结果\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：用spark执行以下逻辑：找到数据行数、列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:9\n",
      "列数:7\n"
     ]
    }
   ],
   "source": [
    "# 统计行数\n",
    "print(f'行数:{test.count()}')\n",
    "\n",
    "# 统计列数\n",
    "print(f'列数:{len(test.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：用spark筛选class为1的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用fliter过滤样本\n",
    "test.filter(test['class'] == '1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 过滤条件\n",
    "f1 = test['language'] > 90\n",
    "f2 = test['math'] > 90\n",
    "\n",
    "# 使用fliter过滤样本\n",
    "test.filter(f1|f2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2：PySpark数据统计\n",
    "\n",
    "- 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "- 步骤2：将读取的进行保存，表头也需要保存\n",
    "- 步骤3：分析每列的类型，取值个数\n",
    "- 步骤4：分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：读取文件\n",
    "\n",
    "文件链接：https://cdn.coggle.club/Pokemon.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# 从url读取数据\n",
    "spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：将读取的进行保存，表头也需要保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据进行保存\n",
    "df.write.csv(\"./data/Pokemon.csv\", header=True, mode=\"overwrite\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：分析每列的类型，取值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type 1: string (nullable = true)\n",
      " |-- Type 2: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- HP: integer (nullable = true)\n",
      " |-- Attack: integer (nullable = true)\n",
      " |-- Defense: integer (nullable = true)\n",
      " |-- Sp Atk: integer (nullable = true)\n",
      " |-- Sp Def: integer (nullable = true)\n",
      " |-- Speed: integer (nullable = true)\n",
      " |-- Generation: integer (nullable = true)\n",
      " |-- Legendary: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 每列类型\n",
    "df.printSchema()    # df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+---------+------+----------+-----------+----------+----------+---------+--------------+-------------+\n",
      "|Name_num|Type 1_num|Type 2_num|Total_num|HP_num|Attack_num|Defense_num|Sp Atk_num|Sp Def_num|Speed_num|Generation_num|Legendary_num|\n",
      "+--------+----------+----------+---------+------+----------+-----------+----------+----------+---------+--------------+-------------+\n",
      "|     799|        18|        18|      200|    94|       111|        103|       105|        92|      108|             6|            2|\n",
      "+--------+----------+----------+---------+------+----------+-----------+----------+----------+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "# 统计各列取值个数\n",
    "df.agg(*[(fn.count_distinct(c)).alias(c+'_num') for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "|Name_null|Type 1_null|Type 2_null|Total_null|HP_null|Attack_null|Defense_null|Sp Atk_null|Sp Def_null|Speed_null|Generation_null|Legendary_null|\n",
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "|        0|          0|        386|         0|      0|          0|           0|          0|          0|         0|              0|             0|\n",
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计各列缺失值数量\n",
    "df.agg(*[(fn.count('*') - fn.count(c)).alias(c+'_null') for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务3：PySpark分组聚合\n",
    "\n",
    "- 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "- 步骤2：学习groupby分组聚合的使用\n",
    "- 步骤3：学习agg分组聚合的使用\n",
    "- 步骤4：学习transform的使用\n",
    "- 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkFiles\n",
    "\n",
    "# # 从url读取数据\n",
    "# spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "# df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "# df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "# df = df.withColumnRenamed('Sp. Def', 'Sp Def')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：学习groupby分组聚合的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html#pyspark.sql.DataFrame.groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：学习agg分组聚合的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：学习transform的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.transform.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  Type 1|          avg(HP)|\n",
      "+--------+-----------------+\n",
      "|   Water|          72.0625|\n",
      "|  Poison|            67.25|\n",
      "|   Steel|65.22222222222223|\n",
      "|    Rock|65.36363636363636|\n",
      "|     Ice|             72.0|\n",
      "|   Ghost|          64.4375|\n",
      "|   Fairy|74.11764705882354|\n",
      "| Psychic|70.63157894736842|\n",
      "|  Dragon|          83.3125|\n",
      "|  Flying|            70.75|\n",
      "|     Bug|56.88405797101449|\n",
      "|Electric|59.79545454545455|\n",
      "|    Fire|69.90384615384616|\n",
      "|  Ground|         73.78125|\n",
      "|    Dark|66.80645161290323|\n",
      "|Fighting|69.85185185185185|\n",
      "|   Grass|67.27142857142857|\n",
      "|  Normal|77.27551020408163|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组统计\n",
    "# df.groupBy('Type 1').avg('HP').show()  # df.groupBy('Type 1').mean('HP').show()\n",
    "df.groupBy('Type 1').agg({'HP':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务4：SparkSQL基础语法\n",
    "\n",
    "- 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "- 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "- 步骤3：使用Spark SQL完成任务3的分组统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "\n",
    "筛选test数据集中 language>90 或 math>90 的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.SQL风格\n",
    "test.createOrReplaceTempView(\"test\")    # 注册临时表\n",
    "spark.sql(\"SELECT * FROM test WHERE language>90 or math>90\").show()\n",
    "\n",
    "# # 2.DSL风格——领域特定语言\n",
    "# test.where(\"language>90 or math>90\").show()\n",
    "# test.filter(\"language>90 or math>90\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "\n",
    "统计各列缺失值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Type 2_missing|\n",
      "+--------------+\n",
      "|           386|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. SQL风格——以某列为例\n",
    "df.createOrReplaceTempView(\"pokemon\")   # 将DataFrame注册为临时表，如果存在进行替换\n",
    "spark.sql(\"SELECT SUM(CASE WHEN `Type 2` IS NULL THEN 1 END) AS `Type 2_missing` FROM pokemon\").show()\n",
    "\n",
    "# # 2.DSL风格\n",
    "# df.agg(*[(fn.count('*') - fn.count(c)).alias(c+'_null') for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：使用Spark SQL完成任务3的分组统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  Type 1|          avg(HP)|\n",
      "+--------+-----------------+\n",
      "|   Water|          72.0625|\n",
      "|  Poison|            67.25|\n",
      "|   Steel|65.22222222222223|\n",
      "|    Rock|65.36363636363636|\n",
      "|     Ice|             72.0|\n",
      "|   Ghost|          64.4375|\n",
      "|   Fairy|74.11764705882354|\n",
      "| Psychic|70.63157894736842|\n",
      "|  Dragon|          83.3125|\n",
      "|  Flying|            70.75|\n",
      "|     Bug|56.88405797101449|\n",
      "|Electric|59.79545454545455|\n",
      "|    Fire|69.90384615384616|\n",
      "|  Ground|         73.78125|\n",
      "|    Dark|66.80645161290323|\n",
      "|Fighting|69.85185185185185|\n",
      "|   Grass|67.27142857142857|\n",
      "|  Normal|77.27551020408163|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.SQL风格\n",
    "spark.sql(\"SELECT `Type 1`, AVG(HP) FROM pokemon GROUP BY `Type 1` \").show()\n",
    "\n",
    "# # 2.DSL风格\n",
    "# df.groupBy('Type 1').agg({'HP':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务5：SparkML基础：数据编码\n",
    "\n",
    "- 步骤1：学习Spark ML中数据编码模块\n",
    "- 步骤2：读取文件Pokemon.csv，理解数据字段含义\n",
    "- 步骤3：将其中的类别属性使用onehotencoder\n",
    "- 步骤4：对其中的数值属性字段使用minmaxscaler\n",
    "- 步骤5：对编码后的属性使用pca进行降维（维度可以自己选择）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：学习Spark ML中数据编码模块\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#feature\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：读取文件Pokemon.csv，理解数据字段含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkFiles\n",
    "\n",
    "# # 从url读取数据\n",
    "# spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "# df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "# df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "# df = df.withColumnRenamed('Sp. Def', 'Sp Def')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：将其中的类别属性使用onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+----------+--------------+--------------+-------------+\n",
      "|                Name|Type 1|Type 2|Generation|      Type1Vec|      Type2Vec|GenerationVec|\n",
      "+--------------------+------+------+----------+--------------+--------------+-------------+\n",
      "|           Bulbasaur| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|             Ivysaur| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|            Venusaur| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|VenusaurMega Venu...| Grass|Poison|         1|(18,[2],[1.0])|(18,[2],[1.0])|(6,[1],[1.0])|\n",
      "|          Charmander|  Fire|  null|         1|(18,[5],[1.0])|    (18,[],[])|(6,[1],[1.0])|\n",
      "+--------------------+------+------+----------+--------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# 由于OneHotEncoder不支持string类型，得先对特征进行索引化\n",
    "stringIndexer = StringIndexer(inputCols=['Type 1', 'Type 2'],\n",
    "                            outputCols=['Type1_index', 'Type2_index'],\n",
    "                            stringOrderType='frequencyDesc')\n",
    "indexed = stringIndexer.setHandleInvalid(\"keep\").fit(df).transform(df)\n",
    "\n",
    "# OneHot编码\n",
    "onehotEncoder = OneHotEncoder(inputCols=['Type1_index', 'Type2_index', 'Generation'],\n",
    "                        outputCols=['Type1Vec', 'Type2Vec', 'GenerationVec'])\n",
    "encoded = onehotEncoder.fit(indexed).transform(indexed)\n",
    "\n",
    "encoded.select(['Name', 'Type 1', 'Type 2', 'Generation', 'Type1Vec', 'Type2Vec', 'GenerationVec']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：对其中的数值属性字段使用minmaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                Name|        Total_scaled|           HP_scaled|       Attack_scaled|      Defense_scaled|       Sp Atk_scaled|       Sp Def_scaled|        Speed_scaled|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           Bulbasaur|              [0.23]|[0.1732283464566929]|[0.23783783783783...|[0.19555555555555...|[0.29891304347826...|[0.2142857142857143]|[0.22857142857142...|\n",
      "|             Ivysaur|             [0.375]|[0.23228346456692...|[0.3081081081081081]|[0.2577777777777778]|[0.3804347826086956]|[0.28571428571428...|[0.3142857142857143]|\n",
      "|            Venusaur|[0.5750000000000001]|[0.3110236220472441]|[0.41621621621621...|[0.3466666666666667]|[0.4891304347826087]| [0.380952380952381]|[0.42857142857142...|\n",
      "|VenusaurMega Venu...|[0.7416666666666667]|[0.3110236220472441]|[0.5135135135135136]|[0.5244444444444445]|[0.6086956521739131]|[0.4761904761904762]|[0.42857142857142...|\n",
      "|          Charmander|[0.21500000000000...|[0.14960629921259...|[0.25405405405405...|[0.1688888888888889]|[0.2717391304347826]|[0.14285714285714...|[0.34285714285714...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 由于MinMaxScalar只能对单行进行归一化，因此要利用Pipeline来对转换列表内的每个属性进行转换\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "columns_to_scale = ['Total', 'HP', 'Attack', 'Defense', 'Sp Atk', 'Sp Def', 'Speed']\n",
    "# 向量化要转换的属性\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col+'_vec') for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col+'_vec', outputCol=col+'_scaled') for col in columns_to_scale]\n",
    "# 定义管道\n",
    "pipeline = Pipeline(stages=assemblers+scalers)\n",
    "scaledDf = pipeline.fit(encoded).transform(encoded)\n",
    "\n",
    "scaledDf.select(['Name', 'Total_scaled', 'HP_scaled', 'Attack_scaled', 'Defense_scaled', 'Sp Atk_scaled', 'Sp Def_scaled', 'Speed_scaled']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：对编码后的属性使用pca进行降维（维度可以自己选择）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                Name|        pcaed_vector|\n",
      "+--------------------+--------------------+\n",
      "|           Bulbasaur|[-0.5779908488663...|\n",
      "|             Ivysaur|[-0.8036261416372...|\n",
      "|            Venusaur|[-1.1159009089467...|\n",
      "|VenusaurMega Venu...|[-1.3790872076314...|\n",
      "|          Charmander|[-0.5695573835440...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA \n",
    "\n",
    "# 利用降维后的种族值做PCA主成分分析\n",
    "columns_to_pca = ['Total_scaled', 'HP_scaled', 'Attack_scaled', 'Defense_scaled', 'Sp Atk_scaled', 'Sp Def_scaled', 'Speed_scaled']\n",
    "# 将需要处理的变量以向量形式表示\n",
    "vecAssembler = VectorAssembler(inputCols=columns_to_pca, outputCol='vector_to_pca')\n",
    "pcaDf = vecAssembler.transform(scaledDf)\n",
    "# PCA主成分分析\n",
    "pcaModel = PCA(k=2, inputCol='vector_to_pca', outputCol='pcaed_vector')\n",
    "pcaedDf = pcaModel.fit(pcaDf).transform(pcaDf)\n",
    "\n",
    "pcaedDf.select(['Name', 'pcaed_vector']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务6：SparkML基础：分类模型\n",
    "\n",
    "- 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder\n",
    "- 步骤2：导入合适的标签评价指标，说出选择的原因？\n",
    "- 步骤3：选择至少3种分类方法，完成训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Type1_index|\n",
      "+-----------+\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        5.0|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 之前已经通过StringEncoder转换为标签\n",
    "pcaedDf.select('Type1_index').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：导入合适的标签评价指标，说出选择的原因？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择使用的标签评价指标是：准确率（Accuracy），因为能够最直观展示模型的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：选择至少3种分类方法，完成训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：(607, 3)\n",
      "测试集：(193, 3)\n"
     ]
    }
   ],
   "source": [
    "# 用于训练的数据集\n",
    "data_set = pcaedDf.select(['Type1_index', 'pcaed_vector', 'vector_to_pca'])\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = data_set.randomSplit([0.75, 0.25], seed=2022)\n",
    "# 展示训练集和测试集容量\n",
    "print(f'训练集：({train_df.count()}, {len(train_df.columns)})')\n",
    "print(f'测试集：({test_df.count()}, {len(test_df.columns)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归（Logistic Regression）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wengz\\anaconda3\\lib\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[1.82187011299118...|[0.10250281517390...|       8.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[1.83790554968661...|[0.10855677053307...|       4.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[1.89809165873014...|[0.11458183706839...|       4.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[1.98460479629409...|[0.07894981842175...|      14.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[1.91438752031794...|[0.08969628188617...|       4.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 构建模型\n",
    "logi_model = LogisticRegression(featuresCol='pcaed_vector', labelCol='Type1_index').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_logi = logi_model.evaluate(train_df).predictions\n",
    "test_result_logi = logi_model.evaluate(test_df).predictions\n",
    "\n",
    "test_result_logi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：13.23%\n",
      "测试集预测准确率：14.91%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_logi = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_logi)\n",
    "accu_test_logi = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_logi)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_logi:.2%}\\n测试集预测准确率：{accu_test_logi:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树（Decision Trees）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[6.0,5.0,3.0,1.0,...|[0.1,0.0833333333...|       8.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[6.0,5.0,3.0,1.0,...|[0.1,0.0833333333...|       8.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[9.0,11.0,4.0,4.0...|[0.12676056338028...|       5.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[0.0,0.0,0.0,2.0,...|[0.0,0.0,0.0,0.33...|      14.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[0.0,2.0,1.0,3.0,...|[0.0,0.125,0.0625...|       6.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# 构建模型\n",
    "dt_model = DecisionTreeClassifier(maxDepth=5, featuresCol='pcaed_vector', labelCol='Type1_index').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_dt = dt_model.transform(train_df)\n",
    "test_result_dt = dt_model.transform(test_df)\n",
    "\n",
    "test_result_dt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：20.07%\n",
      "测试集预测准确率：15.80%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_dt = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_dt)\n",
    "accu_test_dt = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_dt)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_dt:.2%}\\n测试集预测准确率：{accu_test_dt:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林(Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[1.27372288099372...|[0.06368614404968...|       8.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[1.27372288099372...|[0.06368614404968...|       8.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[2.07797424005050...|[0.10389871200252...|       5.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[0.71468457311236...|[0.03573422865561...|      14.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[0.86993586690458...|[0.04349679334522...|      11.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 构建模型\n",
    "rf_model = RandomForestClassifier(maxDepth=5, featuresCol='pcaed_vector', labelCol='Type1_index').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_rf = rf_model.transform(train_df)\n",
    "test_result_rf = rf_model.transform(test_df)\n",
    "\n",
    "test_result_rf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：29.73%\n",
      "测试集预测准确率：14.66%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_rf = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_rf)\n",
    "accu_test_rf = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_rf)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_rf:.2%}\\n测试集预测准确率：{accu_test_rf:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯（Naive Bayes）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Type1_index|        pcaed_vector|       vector_to_pca|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        0.0|[-1.5643373790201...|[0.83333333333333...|[-9.7607238618814...|[0.13189059172286...|       0.0|\n",
      "|        0.0|[-1.5144079854545...|[0.81666666666666...|[-9.6392469119504...|[0.13153579197242...|       0.0|\n",
      "|        0.0|[-1.2820243706233...|[0.63333333333333...|[-8.2230245952990...|[0.13073681400775...|       0.0|\n",
      "|        0.0|[-1.2337665270258...|[0.68333333333333...|[-8.4232867450927...|[0.13386207166064...|       0.0|\n",
      "|        0.0|[-1.1721366811238...|[0.58333333333333...|[-7.8646584468633...|[0.13059304731829...|       0.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# 构建模型\n",
    "bayes_model = NaiveBayes(smoothing=1.0, featuresCol='vector_to_pca', labelCol='Type1_index', modelType='multinomial').fit(train_df)\n",
    "# 预测结果\n",
    "train_result_bayes = bayes_model.transform(train_df)\n",
    "test_result_bayes = bayes_model.transform(test_df)\n",
    "\n",
    "test_result_bayes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测准确率：8.83%\n",
      "测试集预测准确率：9.39%\n"
     ]
    }
   ],
   "source": [
    "# 结果评估\n",
    "accu_train_bayes = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(train_result_bayes)\n",
    "accu_test_bayes = MulticlassClassificationEvaluator(labelCol='Type1_index').evaluate(test_result_bayes)\n",
    "\n",
    "print(f\"训练集预测准确率：{accu_train_bayes:.2%}\\n测试集预测准确率：{accu_test_bayes:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务7：SparkML基础：聚类模型\n",
    "\n",
    "- 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder\n",
    "- 步骤2：使用kmeans对宝可梦进行聚类，使用肘部法选择合适聚类个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：继续任务5的步骤，假设Type 1为标签，将其进行labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Type1_index|\n",
      "+-----------+\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        2.0|\n",
      "|        5.0|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 之前已经通过StringEncoder转换为标签\n",
    "pcaedDf.select('Type1_index').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：使用kmeans对宝可梦进行聚类，使用肘部法选择合适聚类个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# 根据不同K值进行聚类\n",
    "errors = []\n",
    "silhouettes = []\n",
    "for k in range(2, 10):\n",
    "    kmeans_model = KMeans(featuresCol='pcaed_vector', k=k, seed=2022).fit(data_set)\n",
    "    kmeans_results = kmeans_model.transform(data_set)\n",
    "    # 簇内误方差\n",
    "    error = kmeans_model.summary.trainingCost\n",
    "    # 轮廓系数\n",
    "    silhouette = ClusteringEvaluator(predictionCol='prediction', featuresCol='pcaed_vector', metricName='silhouette').evaluate(kmeans_results)\n",
    "    errors.append(error)\n",
    "    silhouettes.append(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 作图根据肘部原则选择 K = 4or5\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(2, 10)\n",
    "plt.plot(x, errors, 'o-', color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务8：Spark RDD\n",
    "\n",
    "- 步骤1：学习资料\n",
    "- 步骤2：使用RDD functions完成任务2的统计逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：学习资料\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：使用RDD functions完成任务2的统计逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Bulbasaur', Type 1='Grass', Type 2='Poison', Total=318, HP=45, Attack=49, Defense=49, Sp Atk=65, Sp Def=65, Speed=45, Generation=1, Legendary=False),\n",
       " Row(Name='Ivysaur', Type 1='Grass', Type 2='Poison', Total=405, HP=60, Attack=62, Defense=63, Sp Atk=80, Sp Def=80, Speed=60, Generation=1, Legendary=False),\n",
       " Row(Name='Venusaur', Type 1='Grass', Type 2='Poison', Total=525, HP=80, Attack=82, Defense=83, Sp Atk=100, Sp Def=100, Speed=80, Generation=1, Legendary=False),\n",
       " Row(Name='VenusaurMega Venusaur', Type 1='Grass', Type 2='Poison', Total=625, HP=80, Attack=100, Defense=123, Sp Atk=122, Sp Def=120, Speed=80, Generation=1, Legendary=False),\n",
       " Row(Name='Charmander', Type 1='Fire', Type 2=None, Total=309, HP=39, Attack=52, Defense=43, Sp Atk=60, Sp Def=50, Speed=65, Generation=1, Legendary=False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame格式数据转换为RDD格式\n",
    "rdd = df.rdd\n",
    "\n",
    "# 展示前5条数据\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>列名</th>\n",
       "      <th>类型</th>\n",
       "      <th>取值个数</th>\n",
       "      <th>缺失值个数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Name</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Type 1</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Type 2</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "      <td>19</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Attack</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Defense</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sp Atk</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sp Def</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Speed</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Generation</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Legendary</td>\n",
       "      <td>&lt;class 'bool'&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            列名              类型  取值个数  缺失值个数\n",
       "0         Name   <class 'str'>   799      0\n",
       "1       Type 1   <class 'str'>    18      0\n",
       "2       Type 2   <class 'str'>    19    386\n",
       "3        Total   <class 'int'>   200      0\n",
       "4           HP   <class 'int'>    94      0\n",
       "5       Attack   <class 'int'>   111      0\n",
       "6      Defense   <class 'int'>   103      0\n",
       "7       Sp Atk   <class 'int'>   105      0\n",
       "8       Sp Def   <class 'int'>    92      0\n",
       "9        Speed   <class 'int'>   108      0\n",
       "10  Generation   <class 'int'>     6      0\n",
       "11   Legendary  <class 'bool'>     2    735"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计每列的类型、取值个数、是否包含缺失值\n",
    "columns = rdd.top(1)[0].asDict().keys()\n",
    "list_col = []\n",
    "\n",
    "for c in columns:\n",
    "    # 类型\n",
    "    type_c = type(rdd.top(1)[0][c])\n",
    "    # 取值个数\n",
    "    cnt = rdd.groupBy(lambda x: x[c]).count()\n",
    "    # 缺失值个数\n",
    "    cntNone = rdd.count() - rdd.filter(lambda x: x[c]).count()\n",
    "    # 加到df里\n",
    "    dict_c = {'列名':c, '类型':type_c, '取值个数':cnt, '缺失值个数':cntNone}\n",
    "    # 加入列表\n",
    "    list_col.append(dict_c)\n",
    "\n",
    "df_des = pd.DataFrame(list_col)\n",
    "df_des\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务9：Spark Streaming\n",
    "\n",
    "- 步骤1：学习资料\n",
    "- 步骤2：读取文件\n",
    "- 步骤3：使用filter筛选行不包含Grass的文本\n",
    "- 步骤4：使用flatmap对文本行进行拆分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：学习资料\n",
    "\n",
    "https://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：读取文件\n",
    "\n",
    "https://cdn.coggle.club/Pokemon.csv为textFileStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.streaming import StreamingContext\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"CrossCorrelation\").getOrCreate()\n",
    "# ssc = StreamingContext(spark.sparkContext, 1)\n",
    "# ds = ssc.textFileStream(\"./test_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：使用filter筛选行不包含Grass的文本"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48de46c395bdbfc127f71df84d0b8cef7bb4c63b5f8ed81eab1950674a230645"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

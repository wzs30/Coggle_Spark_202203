{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark基础教程\n",
    "\n",
    "Spark是一个快速和通用的大数据引擎，可以通俗的理解成一个分布式的大数据处理框架，允许用户将Spark部署在大量廉价的硬件之上，形成集群。Spark使用scala 实现，提供了 JAVA, Python，R等语言的调用接口。在本次学习我们将学习如何使用Spark清洗数据，并进行基础的特征工程操作，帮助大家掌握基础PySpark技能。\n",
    "\n",
    "学习资料：\n",
    "\n",
    "https://spark.apache.org/docs/latest/quick-start.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://github.com/apache/spark/tree/4f25b3f712/examples/src/main/python\n",
    "\n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "https://www.tutorialspoint.com/pyspark/index.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1：PySpark数据处理\n",
    "\n",
    "- 步骤1：使用Python链接Spark环境\n",
    "- 步骤2：创建dateframe数据\n",
    "- 步骤3：用spark执行以下逻辑：找到数据行数、列数\n",
    "- 步骤4：用spark筛选class为1的样本\n",
    "- 步骤5：用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：使用Python链接Spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加此代码，进行spark初始化\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：创建dateframe数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   002|    2|      87|  81|     90|    83|      83|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "|   004|    2|      65|  87|     94|    73|      88|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   006|    3|      84|  82|     85|    73|      99|\n",
      "|   007|    3|      56|  76|     63|    72|      87|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "|   009|    2|      63|  72|     87|    98|      64|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建原始数据 \n",
    "test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "                                ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "                                ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "                            ['number','class','language','math','english','physic','chemical'])\n",
    "# 展示结果\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：用spark执行以下逻辑：找到数据行数、列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:9\n",
      "列数:7\n"
     ]
    }
   ],
   "source": [
    "# 统计行数\n",
    "print(f'行数:{test.count()}')\n",
    "\n",
    "# 统计列数\n",
    "print(f'列数:{len(test.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：用spark筛选class为1的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用fliter过滤样本\n",
    "test.filter(test['class'] == '1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 过滤条件\n",
    "f1 = test['language'] > 90\n",
    "f2 = test['math'] > 90\n",
    "\n",
    "# 使用fliter过滤样本\n",
    "test.filter(f1|f2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2：PySpark数据统计\n",
    "\n",
    "- 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "- 步骤2：将读取的进行保存，表头也需要保存\n",
    "- 步骤3：分析每列的类型，取值个数\n",
    "- 步骤4：分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：读取文件\n",
    "\n",
    "文件链接：https://cdn.coggle.club/Pokemon.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# 从url读取数据\n",
    "spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|    80|    65|   80|         1|    false|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|    50|    64|   43|         1|    false|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|    65|    80|   58|         1|    false|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|    85|   105|   78|         1|    false|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|   135|   115|   78|         1|    false|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|    20|    20|   45|         1|    false|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|    25|    25|   30|         1|    false|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：将读取的进行保存，表头也需要保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据进行保存\n",
    "df.write.csv(\"./data/Pokemon.csv\", header=True, mode=\"overwrite\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：分析每列的类型，取值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Type 1: string (nullable = true)\n",
      " |-- Type 2: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- HP: integer (nullable = true)\n",
      " |-- Attack: integer (nullable = true)\n",
      " |-- Defense: integer (nullable = true)\n",
      " |-- Sp Atk: integer (nullable = true)\n",
      " |-- Sp Def: integer (nullable = true)\n",
      " |-- Speed: integer (nullable = true)\n",
      " |-- Generation: integer (nullable = true)\n",
      " |-- Legendary: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 每列类型\n",
    "df.printSchema()\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+------+------------------+------------------+-----------------+------------------+----------------+-----------------+------------------+------------------+\n",
      "|summary|            Name|Type 1|Type 2|             Total|                HP|           Attack|           Defense|          Sp Atk|           Sp Def|             Speed|        Generation|\n",
      "+-------+----------------+------+------+------------------+------------------+-----------------+------------------+----------------+-----------------+------------------+------------------+\n",
      "|  count|             800|   800|   414|               800|               800|              800|               800|             800|              800|               800|               800|\n",
      "|   mean|            null|  null|  null|          435.1025|          69.25875|         79.00125|           73.8425|           72.82|          71.9025|           68.2775|           3.32375|\n",
      "| stddev|            null|  null|  null|119.96303975551908|25.534669032332076|32.45736586949845|31.183500559332924|32.7222941688016|27.82891579711746|29.060473717161447|1.6612904004849451|\n",
      "|    min|       Abomasnow|   Bug|   Bug|               180|                 1|                5|                 5|              10|               20|                 5|                 1|\n",
      "|    max|Zygarde50% Forme| Water| Water|               780|               255|              190|               230|             194|              230|               180|                 6|\n",
      "+-------+----------------+------+------+------------------+------------------+-----------------+------------------+----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据描述性统计\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "|Name_null|Type 1_null|Type 2_null|Total_null|HP_null|Attack_null|Defense_null|Sp Atk_null|Sp Def_null|Speed_null|Generation_null|Legendary_null|\n",
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "|        0|          0|        386|         0|      0|          0|           0|          0|          0|         0|              0|             0|\n",
      "+---------+-----------+-----------+----------+-------+-----------+------------+-----------+-----------+----------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "# 统计各列缺失值数量\n",
    "df.agg(*[(fn.count('*') - fn.count(c)).alias(c+'_null') for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务3：PySpark分组聚合\n",
    "\n",
    "- 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "- 步骤2：学习groupby分组聚合的使用\n",
    "- 步骤3：学习agg分组聚合的使用\n",
    "- 步骤4：学习transform的使用\n",
    "- 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|    80|    65|   80|         1|    false|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|    50|    64|   43|         1|    false|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|    65|    80|   58|         1|    false|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|    85|   105|   78|         1|    false|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|   135|   115|   78|         1|    false|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|    20|    20|   45|         1|    false|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|    25|    25|   30|         1|    false|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkFiles\n",
    "\n",
    "# # 从url读取数据\n",
    "# spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "# df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "# df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "# df = df.withColumnRenamed('Sp. Def', 'Sp Def')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：学习groupby分组聚合的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html#pyspark.sql.DataFrame.groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：学习agg分组聚合的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4：学习transform的使用\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.transform.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Type 1='Water', avg(HP)=72.0625),\n",
       " Row(Type 1='Poison', avg(HP)=67.25),\n",
       " Row(Type 1='Steel', avg(HP)=65.22222222222223),\n",
       " Row(Type 1='Rock', avg(HP)=65.36363636363636),\n",
       " Row(Type 1='Ice', avg(HP)=72.0),\n",
       " Row(Type 1='Ghost', avg(HP)=64.4375),\n",
       " Row(Type 1='Fairy', avg(HP)=74.11764705882354),\n",
       " Row(Type 1='Psychic', avg(HP)=70.63157894736842),\n",
       " Row(Type 1='Dragon', avg(HP)=83.3125),\n",
       " Row(Type 1='Flying', avg(HP)=70.75),\n",
       " Row(Type 1='Bug', avg(HP)=56.88405797101449),\n",
       " Row(Type 1='Electric', avg(HP)=59.79545454545455),\n",
       " Row(Type 1='Fire', avg(HP)=69.90384615384616),\n",
       " Row(Type 1='Ground', avg(HP)=73.78125),\n",
       " Row(Type 1='Dark', avg(HP)=66.80645161290323),\n",
       " Row(Type 1='Fighting', avg(HP)=69.85185185185185),\n",
       " Row(Type 1='Grass', avg(HP)=67.27142857142857),\n",
       " Row(Type 1='Normal', avg(HP)=77.27551020408163)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.groupBy('Type 1').avg('HP').collect()  # df.groupBy('Type 1').mean('HP').collect()\n",
    "df.groupBy('Type 1').agg({'HP':'mean'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务4：SparkSQL基础语法\n",
    "\n",
    "- 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "- 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "- 步骤3：使用Spark SQL完成任务3的分组统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤1：使用Spark SQL完成任务1里面的数据筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number='001', class='1', language=100, math=87, english=67, physic=83, chemical=98),\n",
       " Row(number='003', class='3', language=86, math=91, english=83, physic=89, chemical=63)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用spark筛选test数据集中 language>90 或 math>90 的样本\n",
    "# # 过滤条件\n",
    "# f1 = test['language'] > 90\n",
    "# f2 = test['math'] > 90\n",
    "# # 使用fliter过滤样本\n",
    "# test.filter(f1|f2).show()\n",
    "\n",
    "test.createOrReplaceTempView(\"test\")\n",
    "spark.sql(\"SELECT * FROM test WHERE language>90 or math>90\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3：使用Spark SQL完成任务3的分组统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48de46c395bdbfc127f71df84d0b8cef7bb4c63b5f8ed81eab1950674a230645"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
